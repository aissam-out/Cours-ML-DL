{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Les Algorithmes Classiques du Machine Learning"
      ],
      "metadata": {
        "id": "Gqe-tnne-O-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bienvenue dans ce notebook dédié aux algorithmes classiques du Machine Learning. L'objectif est de comprendre le pourquoi et le comment de chaque algorithme, avec des explications détaillées et des implémentations en Python.\n"
      ],
      "metadata": {
        "id": "_-4nr6Lr-RdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Régression Linéaire\n"
      ],
      "metadata": {
        "id": "kfX0Etwq-WL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "La régression linéaire est l'un des algorithmes les plus simples et les plus utilisés en apprentissage supervisé pour les tâches de prédiction de valeurs continues. Elle établit une relation linéaire entre une variable dépendante (cible) et une ou plusieurs variables indépendantes (caractéristiques).\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Simplicité : Facile à interpréter et à expliquer.\n",
        "- Rapidité : Rapide à entraîner, même sur de grands jeux de données.\n",
        "- Base pour d'autres algorithmes : Sert de fondation pour des modèles plus complexes.\n",
        "\n",
        "### Comment ça marche ?\n",
        "La régression linéaire cherche à ajuster une ligne (ou un hyperplan en dimensions supérieures) qui minimise la somme des carrés des distances (erreurs) entre les points de données réels et les prédictions du modèle.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le jeu de données Diabetes de scikit-learn pour prédire la progression de la maladie un an plus tard en fonction de certaines caractéristiques."
      ],
      "metadata": {
        "id": "WqCuOPEs-dDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zF_RMib-I8z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pour ignorer les avertissements inutiles\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer des données de régression linéaire avec bruit\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "# Conversion en DataFrame pour une meilleure manipulation\n",
        "data = pd.DataFrame({'Feature': X.flatten(), 'Target': y})"
      ],
      "metadata": {
        "id": "aAwzCJpg-xII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation de la relation entre la caractéristique et la cible\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=data, x='Feature', y='Target')\n",
        "plt.title('Relation entre la Caractéristique et la Cible')\n",
        "plt.xlabel('Caractéristique')\n",
        "plt.ylabel('Cible')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "slFezOkuAwZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Division en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature']], data['Target'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "e8KLmX3LAwsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création et entraînement du modèle\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Affichage des coefficients\n",
        "# Y = β0 + X * β1\n",
        "print(\"Coefficient (β1) :\", model.coef_[0])\n",
        "print(\"Ordonnée à l'origine (β0) :\", model.intercept_)"
      ],
      "metadata": {
        "id": "yGshHQE8A1L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "gniAFhUyA294"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des Résultats\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_test, y_test, color='blue', label='Données Réelles')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Prédictions')\n",
        "plt.title('Régression Linéaire - Données Réelles vs Prédictions')\n",
        "plt.xlabel('Caractéristique')\n",
        "plt.ylabel('Cible')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YSs1hTchBWOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Erreur Absolue Moyenne (MAE):', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('Erreur Quadratique Moyenne (MSE):', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Racine de l\\'Erreur Quadratique Moyenne (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "ELWswcWwBZsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La régression linéaire est un outil puissant pour modéliser les relations linéaires entre les variables. En analysant les résidus et en vérifiant les hypothèses, nous pouvons évaluer la fiabilité du modèle. Dans cet exemple, le modèle semble bien ajusté aux données, comme le montrent les métriques de performance et les graphiques.\n"
      ],
      "metadata": {
        "id": "nXoSeyBJCF-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Régression Logistique\n"
      ],
      "metadata": {
        "id": "DHiUgRtdCN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "La régression logistique est un algorithme d'apprentissage supervisé utilisé pour les tâches de classification binaire (et multiclasse avec certaines extensions). Elle modélise la probabilité qu'une instance appartienne à une classe particulière.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "\n",
        "- Interprétabilité : Les coefficients peuvent être interprétés comme l'effet des variables indépendantes sur la probabilité.\n",
        "- Efficacité : Fonctionne bien sur des données linéairement séparables.\n",
        "- Probabilités : Fournit des probabilités pour les prédictions, ce qui est utile pour la prise de décision.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le jeu de données Breast Cancer de scikit-learn pour prédire si une tumeur est maligne ou bénigne."
      ],
      "metadata": {
        "id": "Bb7FgtdWCRkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
      ],
      "metadata": {
        "id": "zaMwRF4SBivf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du jeu de données\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "X = cancer.data\n",
        "y = cancer.target"
      ],
      "metadata": {
        "id": "jEihSSXGBt0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un DataFrame pour une meilleure visualisation\n",
        "df = pd.DataFrame(X, columns=cancer.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Afficher des lignes\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "khmq6VmoCsZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier la distribution des classes\n",
        "df['target'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution des Classes')\n",
        "plt.xlabel('Classe')\n",
        "plt.ylabel('Nombre d\\'échantillons')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xsSe7irxCt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Division en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JiQBWyCzC1_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HNsf9IFXC4NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "mIHdLjV1C50N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matrice de Confusion:\\n\", cm)\n",
        "\n",
        "# Rapport de classification\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Rapport de Classification:\\n\", cr)"
      ],
      "metadata": {
        "id": "12ePqSSDC7gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.xlabel('Prédictions')\n",
        "plt.ylabel('Véritables')\n",
        "plt.title('Matrice de Confusion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7EI4qn4rC9I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La régression logistique est un algorithme simple mais puissant pour les tâches de classification binaire. Il fournit non seulement des prédictions, mais aussi des probabilités associées, ce qui est utile pour la prise de décision."
      ],
      "metadata": {
        "id": "ObdilEIcDQ3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Arbres de Décision"
      ],
      "metadata": {
        "id": "qk06026tDVe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Les arbres de décision sont des modèles non paramétriques utilisés pour les tâches de classification et de régression. Ils apprennent des règles de décision simples inférées des données pour prédire la valeur d'une cible.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Interprétable : Facile à visualiser et à comprendre.\n",
        "- Peut capturer des relations non linéaires.\n",
        "- Peu de préparation des données nécessaire : Pas besoin de normaliser ou de mettre à l'échelle les données.\n",
        "\n",
        "### Comment ça marche ?\n",
        "L'algorithme partitionne récursivement les données en sous-ensembles basés sur les caractéristiques qui fournissent la meilleure séparation selon une mesure. À chaque nœud, l'algorithme choisit la caractéristique qui divise le mieux les données.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le jeu de données Iris pour classer les espèces de fleurs."
      ],
      "metadata": {
        "id": "W7bU3x1ZDboi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "tNp4qObTDLZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du jeu de données\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "3FdjDCDjDkt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Division en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "G5aVXyGDDmjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "zrm3RqGzDoAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation de l'Arbre\n",
        "plt.figure(figsize=(20,10))\n",
        "tree.plot_tree(model, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OAt5VugLDpUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)"
      ],
      "metadata": {
        "id": "bk_zCAAjDrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interprétation\n",
        "- Précision : Indique la proportion de prédictions correctes.\n",
        "- Visualisation de l'Arbre : Permet de comprendre les règles de décision utilisées par le modèle.\n",
        "\n",
        "#### Conclusion\n",
        "Les arbres de décision sont des modèles faciles à interpréter et puissants pour capturer des relations complexes dans les données. Cependant, ils peuvent être sujets au surapprentissage si la profondeur de l'arbre n'est pas contrôlée."
      ],
      "metadata": {
        "id": "NzitZZwFD-IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Forêts Aléatoires"
      ],
      "metadata": {
        "id": "J5XbCI38ENkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Les forêts aléatoires sont un ensemble d'arbres de décision (méthode d'ensemble) où chaque arbre est construit à partir d'un échantillon aléatoire du jeu de données. Elles utilisent la moyenne ou la majorité pour améliorer la précision prédictive et contrôler le surapprentissage.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Puissant et précis : Donne de bonnes performances sur de nombreux problèmes.\n",
        "- Réduit le surapprentissage : En moyenne, les résultats de plusieurs arbres réduisent la variance.\n",
        "- Gère les données manquantes et les variables catégorielles.\n",
        "\n",
        "### Comment ça marche ?\n",
        "Chaque arbre de la forêt est construit à partir d'un échantillon du jeu de données. De plus, lors de la construction de chaque nœud, un sous-ensemble aléatoire des caractéristiques est utilisé pour déterminer la meilleure séparation.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le même jeu de données Iris."
      ],
      "metadata": {
        "id": "pl3yo9wDESXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "qJwEGA_3D2F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "u2c0ndu6Ek49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)"
      ],
      "metadata": {
        "id": "XBcKnewHElj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance des caractéristiques\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Affichage\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Importance des Caractéristiques\")\n",
        "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
        "plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices])\n",
        "plt.xlabel('Caractéristique')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1sGzIznxEn7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interprétation\n",
        "- Précision : Souvent meilleure que celle d'un seul arbre de décision.\n",
        "- Importance des Caractéristiques : Indique quelles caractéristiques sont les plus importantes pour la prédiction.\n",
        "\n",
        "#### Conclusion\n",
        "Les forêts aléatoires améliorent la performance des arbres de décision en réduisant le surapprentissage et en augmentant la généralisation du modèle."
      ],
      "metadata": {
        "id": "pxnkxlYiEt8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Machines à Vecteurs de Support (SVM)"
      ],
      "metadata": {
        "id": "bR-gvf6uE1xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Les SVM sont des modèles supervisés utilisés pour la classification et la régression. Ils cherchent à trouver l'hyperplan qui maximise la marge entre les différentes classes.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Efficace en haute dimension.\n",
        "- Utilise des fonctions noyau : Peut gérer des données non linéaires.\n",
        "- Robuste au surapprentissage : Surtout dans des espaces de grande dimension.\n",
        "\n",
        "### Comment ça marche ?\n",
        "Les SVM trouvent l'hyperplan qui sépare les classes avec la plus grande marge possible. Les points de données les plus proches de l'hyperplan sont appelés vecteurs de support. Les fonctions noyau permettent de transformer les données en un espace de dimension supérieure pour rendre les données linéairement séparables.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le jeu de données Iris pour la classification."
      ],
      "metadata": {
        "id": "mwKZKATgE5zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "o7HxM5z1Ep66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "model = SVC(kernel='linear')\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6Y-vVnWQFOXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)"
      ],
      "metadata": {
        "id": "J28djNPGFP5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualisation (pour 2 caractéristiques)**\n",
        "\n",
        "Pour visualiser les résultats, nous devons réduire les données à 2 caractéristiques."
      ],
      "metadata": {
        "id": "aAHZ2HH4FU7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélection de deux caractéristiques\n",
        "X_reduced = iris.data[:, :2]  # Longueur et largeur des sépales\n",
        "y = iris.target\n",
        "\n",
        "# Division des données\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2)\n",
        "\n",
        "# Entraînement du modèle\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6kseZMQpFRoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'une grille pour la visualisation\n",
        "h = .02  # taille de l'étape dans la grille\n",
        "x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
        "y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Prédictions sur la grille\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Affichage\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.xlabel('Longueur du Sépale')\n",
        "plt.ylabel('Largeur du Sépale')\n",
        "plt.title('SVM avec Noyau Linéaire')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8dcdUU8yFZlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "Les SVM sont des algorithmes puissants pour la classification, en particulier dans des espaces de grande dimension. Les fonctions noyau permettent de gérer des données non linéaires."
      ],
      "metadata": {
        "id": "l-3UO0raFjpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. K-Plus Proches Voisins (KNN)\n"
      ],
      "metadata": {
        "id": "hJVtjLz3Fng6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Le KNN est un algorithme non paramétrique utilisé pour la classification et la régression. Il classe les instances en fonction des classes des K voisins les plus proches dans l'espace des caractéristiques.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Simplicité : Facile à comprendre et à implémenter.\n",
        "- Pas de phase d'entraînement : Les données sont utilisées directement pour la prédiction.\n",
        "- Flexible : Peut capturer des relations complexes.\n",
        "\n",
        "### Comment ça marche ?\n",
        "Pour une nouvelle instance, l'algorithme trouve les K instances du jeu de données les plus proches (selon une métrique de distance, généralement la distance euclidienne) et effectue la prédiction en fonction des labels de ces voisins (par majorité pour la classification ou moyenne pour la régression).\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser le jeu de données Iris."
      ],
      "metadata": {
        "id": "IUxuPNirFsMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "03gPyl1WFcoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jjgsl0eCGcNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Clarification sur le KNN et la Phase d'Entraînement**\n",
        "\n",
        "Dans l'algorithme KNN, il est souvent dit qu'il n'y a pas de véritable phase d'entraînement parce que le modèle ne construit pas de représentation abstraite ou n'apprend pas de paramètres à partir des données d'entraînement comme le font d'autres algorithmes (par exemple, les coefficients en régression linéaire).\n",
        "\n",
        "Cependant, le KNN nécessite de stocker les données d'entraînement pour pouvoir calculer les distances entre les nouvelles instances et les instances d'entraînement lors de la phase de prédiction. Dans scikit-learn, la méthode fit() est utilisée pour stocker ces données."
      ],
      "metadata": {
        "id": "bcaJRMZJGTbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)"
      ],
      "metadata": {
        "id": "7Vh_cgjkF5cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester différents valeurs de K\n",
        "error_rate = []\n",
        "\n",
        "# Boucle sur différentes valeurs de K\n",
        "for i in range(1, 30):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error_rate.append(np.mean(pred_i != y_test))\n",
        "\n",
        "# Visualisation de l'erreur en fonction de K\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,30), error_rate, color='blue', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='red', markersize=10)\n",
        "plt.title('Taux d\\'erreur vs. Valeur de K')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Taux d\\'erreur')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkfG1bsvGjnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interprétation\n",
        "- Précision : Dépend fortement du choix de K.\n",
        "- Choix de K : Un K trop petit peut entraîner un surapprentissage, tandis qu'un K trop grand peut sous-apprendre.\n",
        "\n",
        "#### Conclusion\n",
        "Le KNN est un algorithme simple et efficace, mais il est important de choisir la bonne valeur de K et de prendre en compte que le temps de calcul peut être important pour de grands jeux de données."
      ],
      "metadata": {
        "id": "th3Jb1eaGqHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Boosting"
      ],
      "metadata": {
        "id": "bfQ2wNTqG11s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Le boosting est une méthode d'ensemble qui combine plusieurs modèles faibles pour créer un modèle fort. L'algorithme le plus connu est AdaBoost (Adaptive Boosting).\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Améliore les performances : En combinant des modèles faibles, on obtient de meilleures prédictions.\n",
        "- Réduction du biais : Le boosting se concentre sur les erreurs des modèles précédents.\n",
        "- Flexible : Peut être utilisé avec différents types de modèles faibles.\n",
        "\n",
        "### Comment ça marche ?\n",
        "Le boosting entraîne les modèles séquentiellement, chaque modèle tentant de corriger les erreurs des modèles précédents. Les observations mal prédites sont pondérées plus fortement dans les modèles suivants.\n",
        "\n",
        "### Implémentation en Python\n",
        "Nous allons utiliser AdaBoost avec des arbres de décision faibles."
      ],
      "metadata": {
        "id": "I7L6TLicG256"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pour ignorer les avertissements inutiles\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
      ],
      "metadata": {
        "id": "kobqmmIMGp34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer des données de classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# Conversion en DataFrame pour une meilleure manipulation\n",
        "data = pd.DataFrame(X)\n",
        "data['target'] = y"
      ],
      "metadata": {
        "id": "afWfYafOHFgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les premières lignes du jeu de données\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Hw2MTbkCHHkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier la distribution des classes\n",
        "data['target'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution des Classes')\n",
        "plt.xlabel('Classe')\n",
        "plt.ylabel('Nombre d\\'échantillons')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oZGxSB4oHJTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Séparer les caractéristiques et la cible\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Division en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ozPr96VfHLTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle AdaBoost avec le paramètre correct 'estimator'\n",
        "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                         n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Entraînement du modèle\n",
        "ada.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NY4dpeFvH3mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions sur l'ensemble de test\n",
        "y_pred = ada.predict(X_test)"
      ],
      "metadata": {
        "id": "jTjreYKPH64q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Précision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Précision :\", accuracy)\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matrice de Confusion:\\n\", cm)\n",
        "\n",
        "# Rapport de classification\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Rapport de Classification:\\n\", cr)"
      ],
      "metadata": {
        "id": "S-j_DUP5IS38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Prédictions')\n",
        "plt.ylabel('Véritables')\n",
        "plt.title('Matrice de Confusion - AdaBoost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SI1OZYTFIUnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance des caractéristiques\n",
        "importances = ada.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Affichage\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title(\"Importance des Caractéristiques - AdaBoost\")\n",
        "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
        "plt.xticks(range(X.shape[1]), indices)\n",
        "plt.xlabel('Indices des Caractéristiques')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oODSixKbIaI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. K-Means Clustering"
      ],
      "metadata": {
        "id": "UaP8onod62SY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Le K-Means Clustering est un algorithme d'apprentissage non supervisé utilisé pour regrouper des données similaires en clusters (groupes). Il partitionne les données en K clusters, où chaque observation appartient au cluster avec la moyenne la plus proche.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Exploration des Données : Utile pour découvrir des structures cachées dans les données.\n",
        "- Simplicité : Facile à comprendre et à implémenter.\n",
        "- Efficacité : Rapide pour les grands jeux de données.\n",
        "\n",
        "### Comment ça marche ?\n",
        "- Initialisation : Choisir K centres de clusters initiaux (centroïdes), soit aléatoirement, soit en utilisant des méthodes heuristiques.\n",
        "- Assignation : Affecter chaque point de données au centroïde le plus proche.\n",
        "- Mise à Jour : Calculer la nouvelle position des centroïdes en prenant la moyenne des points assignés à chaque cluster.\n",
        "- Convergence : Répéter les étapes d'assignation et de mise à jour jusqu'à ce que les centroïdes ne changent plus significativement.\n",
        "\n",
        "### Limitations\n",
        "- Choix de K : Le nombre de clusters K doit être spécifié à l'avance.\n",
        "- Sensibilité aux Valeurs Anormales : Les outliers peuvent affecter les résultats.\n",
        "- Forme des Clusters : Suppose que les clusters sont sphériques et de taille similaire."
      ],
      "metadata": {
        "id": "sZskDK0d69EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "OSzKAvaMIc0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer des données avec des blobs (groupes)\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Mise à l'échelle des données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Afficher les données\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], s=50)\n",
        "plt.title('Données Synthétiques')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CGT487-D7O0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Détermination du Nombre Optimal de Clusters (K)**\n",
        "\n",
        "Nous pouvons utiliser la méthode du coude pour déterminer le nombre optimal de clusters."
      ],
      "metadata": {
        "id": "Sb2Gh_IZ7WbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wcss = []  # Within-Cluster Sum of Square\n",
        "\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Visualisation de la méthode du coude\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,11), wcss, marker='o')\n",
        "plt.title('La Méthode du Coude')\n",
        "plt.xlabel('Nombre de Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfFhRjFo7Qyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "Le \"coude\" dans le graphique indique le nombre optimal de clusters. Ici, le coude est autour de K=4."
      ],
      "metadata": {
        "id": "2gt2g41i7jFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle avec K=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X_scaled)"
      ],
      "metadata": {
        "id": "2dzPu8Vc7a8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédictions des clusters\n",
        "clusters = kmeans.predict(X_scaled)\n",
        "\n",
        "# Ajout des clusters au DataFrame\n",
        "data = pd.DataFrame(X_scaled, columns=['Feature 1', 'Feature 2'])\n",
        "data['Cluster'] = clusters"
      ],
      "metadata": {
        "id": "iFDXlnKJ7lyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=data, x='Feature 1', y='Feature 2', hue='Cluster', palette='viridis', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', label='Centroïdes')\n",
        "plt.title('Clusters K-Means')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ZKbPO2F7n-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interprétation\n",
        "Les points sont colorés selon leur cluster assigné, et les centroïdes sont indiqués en rouge. Nous pouvons observer que l'algorithme a correctement identifié les groupes naturels dans les données.\n",
        "\n",
        "#### Évaluation du Modèle\n",
        "Bien que les techniques d'évaluation pour le clustering soient moins directes que pour l'apprentissage supervisé, nous pouvons utiliser des métriques telles que le Coefficient de Silhouette."
      ],
      "metadata": {
        "id": "YkXYHcZC7t9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(X_scaled, clusters)\n",
        "print('Coefficient de Silhouette : {:.2f}'.format(score))"
      ],
      "metadata": {
        "id": "VfGqZ1ZL7pvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interprétation :\n",
        "\n",
        "Le coefficient de silhouette varie entre -1 et 1. Un score proche de 1 indique que les points sont bien assignés à leurs clusters.\n",
        "\n",
        "### Conclusion\n",
        "Le K-Means Clustering est une technique efficace pour regrouper des données non étiquetées en clusters significatifs. Il est important de standardiser les données et de choisir judicieusement le nombre de clusters pour obtenir de bons résultats."
      ],
      "metadata": {
        "id": "Jlw_a8Yy75bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Analyse en Composantes Principales (PCA)"
      ],
      "metadata": {
        "id": "2Bx40K8lGlyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "L'Analyse en Composantes Principales (PCA) est une technique d'apprentissage non supervisé utilisée pour la réduction de dimensionnalité. Elle transforme un ensemble de variables éventuellement corrélées en un ensemble de variables non corrélées appelées composantes principales.\n",
        "\n",
        "### Pourquoi l'utiliser ?\n",
        "- Réduction de la Dimensionnalité : Diminue le nombre de variables tout en conservant le maximum d'information possible.\n",
        "- Visualisation : Permet de visualiser des données haute dimension en 2D ou 3D.\n",
        "- Élimination du Bruit : Réduit le bruit en éliminant les composantes avec une faible variance.\n",
        "- Prétraitement : Peut améliorer les performances des modèles d'apprentissage supervisé en réduisant le surapprentissage.\n",
        "\n",
        "### Comment ça marche ?\n",
        "- Standardisation des Données : Les variables sont mises à l'échelle pour avoir une moyenne de 0 et un écart-type de 1.\n",
        "- Calcul de la Matrice de Covariance : Mesure comment les variables varient ensemble.\n",
        "- Calcul des Valeurs Propres et Vecteurs Propres : Les valeurs propres indiquent l'importance de chaque composante, les vecteurs propres définissent la direction.\n",
        "- Projection : Les données sont projetées sur les composantes principales.\n",
        "\n",
        "### Illustration avec des Données Synthétiques\n",
        "Nous allons créer un jeu de données synthétique en 3 dimensions, où les variables sont fortement corrélées. Le but est de montrer comment le PCA peut réduire la dimensionnalité tout en conservant l'essentiel de l'information."
      ],
      "metadata": {
        "id": "Z8ZcL26pGyOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "xvAio43D72fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixer la seed pour la reproductibilité\n",
        "np.random.seed(42)\n",
        "\n",
        "# Génération de 1000 points\n",
        "n_samples = 1000\n",
        "\n",
        "# Création de variables fortement corrélées\n",
        "X1 = np.random.normal(loc=0.0, scale=1.0, size=n_samples)\n",
        "X2 = X1 * 0.8 + np.random.normal(loc=0.0, scale=0.2, size=n_samples)\n",
        "X3 = X1 * 0.6 + X2 * 0.3 + np.random.normal(loc=0.0, scale=0.2, size=n_samples)\n",
        "\n",
        "# Création d'un DataFrame\n",
        "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3})"
      ],
      "metadata": {
        "id": "iK5arcR1HRWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation en 3D\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(df['X1'], df['X2'], df['X3'], c='skyblue', s=60)\n",
        "ax.set_xlabel('X1')\n",
        "ax.set_ylabel('X2')\n",
        "ax.set_zlabel('X3')\n",
        "plt.title('Données Synthétiques en 3D')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1fwtJLZHULJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrice de corrélation\n",
        "corr_matrix = df.corr()\n",
        "print(\"Matrice de Corrélation :\")\n",
        "print(corr_matrix)\n",
        "\n",
        "# Heatmap de la matrice de corrélation\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Matrice de Corrélation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KPsFodsZHWfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "Les variables X1, X2 et X3 sont fortement corrélées entre elles. Cela signifie qu'il y a une redondance d'information, et que nous pouvons réduire la dimensionnalité sans perdre beaucoup d'information."
      ],
      "metadata": {
        "id": "I0fdiB_qHc-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation des données\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "bUYLeZmPHg9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application du PCA**\n",
        "\n",
        "Nous allons appliquer le PCA pour réduire la dimensionnalité de 3 à 2 composantes principales."
      ],
      "metadata": {
        "id": "_hX09_6dHlNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA avec 2 composantes principales\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Création d'un DataFrame avec les composantes principales\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])"
      ],
      "metadata": {
        "id": "c1DDrTSCHqEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variance expliquée par chaque composante principale\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Variance expliquée par chaque composante :\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")\n",
        "\n",
        "# Variance expliquée cumulative\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "print(\"\\nVariance expliquée cumulative :\")\n",
        "for i, var in enumerate(cumulative_variance):\n",
        "    print(f\"PC1 to PC{i+1}: {var:.4f}\")"
      ],
      "metadata": {
        "id": "jmthq1oVHsLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "La première composante principale (PC1) explique une grande partie de la variance totale, et les deux premières composantes principales expliquent ensemble presque toute la variance. Cela signifie que nous pouvons représenter nos données en 2 dimensions sans perdre beaucoup d'information."
      ],
      "metadata": {
        "id": "Yr9IGj8hHwJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des données projetées sur les deux premières composantes principales\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_df['PC1'], pca_df['PC2'], c='skyblue', s=60)\n",
        "plt.xlabel('Composante Principale 1')\n",
        "plt.ylabel('Composante Principale 2')\n",
        "plt.title('Projection PCA sur les Deux Premières Composantes')\n",
        "plt.axhline(0, color='gray', lw=1)\n",
        "plt.axvline(0, color='gray', lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mal6t_d4H5ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "Nous avons réussi à représenter les données en 2 dimensions tout en conservant l'essentiel de l'information.\n",
        "\n",
        "**Reconstruction des Données**\n",
        "\n",
        "Nous pouvons également reconstruire les données originales à partir des composantes principales pour voir combien d'information a été perdue."
      ],
      "metadata": {
        "id": "pdQLbWxDIAxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruction des données à partir des composantes principales\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Calcul de l'erreur de reconstruction\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "print(f\"Erreur de reconstruction moyenne : {reconstruction_error:.4f}\")"
      ],
      "metadata": {
        "id": "HAdprqX6H-__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "L'erreur de reconstruction est très faible, ce qui signifie que la réduction de dimensionnalité n'a pas entraîné une perte significative d'information.\n",
        "\n",
        "**Visualisation de la Reconstruction**\n",
        "\n",
        "Comparons les données originales et reconstruites pour une variable."
      ],
      "metadata": {
        "id": "Pnb5MHNlIMT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison pour la première variable\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(X_scaled[:,0], label='Donnée Originale')\n",
        "plt.plot(X_reconstructed[:,0], label='Donnée Reconstituée', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Comparaison de la Donnée Originale et Reconstituée (X1)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-xnrDqklIHXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interprétation :**\n",
        "\n",
        "Les courbes des données originales et reconstruites se superposent presque parfaitement, montrant que la réduction de dimensionnalité a préservé l'essentiel de l'information.\n"
      ],
      "metadata": {
        "id": "7D_8ladhIYVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bénéfices du PCA**\n",
        "\n",
        "- Réduction de la Dimensionnalité : Nous sommes passés de 3 à 2 dimensions tout en conservant la majeure partie de l'information (variance).\n",
        "- Visualisation : En réduisant à 2 dimensions, nous pouvons visualiser les données plus facilement.\n",
        "- Élimination de la Redondance : Le PCA élimine la redondance due à la corrélation entre les variables.\n",
        "- Prétraitement pour l'Apprentissage Supervisé : En réduisant le nombre de variables, nous pouvons simplifier les modèles d'apprentissage supervisé et réduire le risque de surapprentissage."
      ],
      "metadata": {
        "id": "rAu_MUFiImVu"
      }
    }
  ]
}